{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rearing Prediction using Support Vector Machines:\n",
    "\n",
    "\n",
    " ![title](rearing_flowchart.png)\n",
    " \n",
    "Here our goal is to predict if the animal model is rearing at each time step given instances of the Marker predictions we derived from Deeplabcut. We will use the Marker predictions file(.csv file) as well as Manual annotations of Rearing(.csv file) to train an Support Vector Machine to learn to predict when the animal is rearing.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from plyfile import PlyData, PlyElement\n",
    "from PIL import Image\n",
    "import math\n",
    "import pandas as pd \n",
    "from sklearn import svm\n",
    "import os\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, we need the prediction output from DeeplabCut as well as ground truth annotations for rearing. The directories below should be modified to include the locations of these files on your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goutham/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Import the Marker prediction file as well as the rearing annotations\n",
    "prediction_dir='/Users/goutham/Documents/Senior_year/research_design/Test7PART1DLC_resnet50_Trial3Mar23shuffle1_10000.csv' \n",
    "rearing_dat='/Users/goutham/Documents/Senior_year/research_design/trial7_rearing_first10min.xlsx'\n",
    "\n",
    "prediction_data=pd.read_csv(prediction_dir,header=1)\n",
    "rearing_gt=pd.read_excel(rearing_dat,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some preprocessing steps. Note that the frame rate for the camera that we used in our study was 15 frames/second and we have done our analysis depending on our start/stop points. This should be modified for different experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "\n",
    "pred_data_train=prediction_data.iloc[1:2175]#this is the 5th minute to 7.5 minutes\n",
    "pred_data_test=prediction_data.iloc[2176:4425] #this is the 7.5th minute to the 10 minute\n",
    "\n",
    "rearing_gt_train=rearing_gt.iloc[4576:6750,1].to_numpy().astype(float) # add 5 second bias 5*15=75 b/c dow started 5 sec late\n",
    "rearing_gt_test=rearing_gt.iloc[6751:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features that we derived are experimentally determined for rearing and showed good classification. We used six total features which include: euclidean distance from snout to tailbase, euclidean distance from snout to right hip, euclidean distance from snout to left hip, and the Deeplabcut models confidence in prediction of the centroid, the right centroid, and left centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Euclidean_distance(x_1,y_1,x_2,y_2):\n",
    "    euc_dist=np.sqrt((((x_1-x_2)**2) + ((y_1-y_2)**2)))\n",
    "    return euc_dist\n",
    "\n",
    "\n",
    "\n",
    "snout_xy_train=pred_data_train[[\"snout\",\"snout.1\"]].to_numpy().astype(float)\n",
    "centroid_xy_train=pred_data_train[[\"rightear\",\"rightear.1\"]].to_numpy().astype(float)\n",
    "tailbase_xy_train=pred_data_train[[\"tailbase\",\"tailbase.1\"]].to_numpy().astype(float)\n",
    "hip_right_xy_train=pred_data_train[[\"righthip\",\"righthip.1\"]].to_numpy().astype(float)\n",
    "hip_left_xy_train=pred_data_train[[\"lefthip\",\"lefthip.1\"]].to_numpy().astype(float)\n",
    "\n",
    "\n",
    "#feeatures\n",
    "distance_snout_tailbase=Euclidean_distance(snout_xy_train[:,0],snout_xy_train[:,1],tailbase_xy_train[:,0],tailbase_xy_train[:,1])\n",
    "distance_snout_right_hip=Euclidean_distance(snout_xy_train[:,0],snout_xy_train[:,1],hip_right_xy_train[:,0],hip_right_xy_train[:,1])\n",
    "distance_snout_left_hip=Euclidean_distance(snout_xy_train[:,0],snout_xy_train[:,1],hip_left_xy_train[:,0],hip_left_xy_train[:,1])\n",
    "prob_centroid=pred_data_train[[\"centroid.2\",\"rightcentroid.2\",\"leftcentroid.2\"]].to_numpy().astype(float)\n",
    "\n",
    "features=np.column_stack((distance_snout_tailbase,distance_snout_right_hip,distance_snout_left_hip,prob_centroid))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a balanced dataset(equal number of instances per class) for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "delete_num=[]\n",
    "negative_instances=[]\n",
    "for i in range(0,len(rearing_gt_train)):\n",
    "    if rearing_gt_train[i]==0:\n",
    "        delete_num.append(i)\n",
    "    else:\n",
    "        negative_instances.append(i)\n",
    "\n",
    "positive_instances_pred=np.delete(features,delete_num,0)\n",
    "positive_rearing=np.delete(rearing_gt_train,delete_num)\n",
    "\n",
    "negative_instances_pred=np.delete(features,negative_instances,0)\n",
    "negative_rearing=np.delete(rearing_gt_train,negative_instances)\n",
    "\n",
    "\n",
    "\n",
    "averages_pred_positive=np.mean(positive_instances_pred, axis=0)\n",
    "averages_pred_non_rearing=np.mean(negative_instances_pred, axis=0)\n",
    "\n",
    "Differences=np.abs(averages_pred_positive-averages_pred_non_rearing)\n",
    "\n",
    "# creating a balanced dataset\n",
    "random.seed(2)\n",
    "training_samples_positive=random.sample(range(0, len(positive_rearing)), int(len(positive_rearing)*0.7)) # first argument is the range of random numbers to chhose from and second is the total number in training\n",
    "training_samples_negative=random.sample(range(0,len(negative_rearing)),int(len(positive_rearing)*0.7))\n",
    "\n",
    "\n",
    "holdout_samples_positive=[]\n",
    "holdout_samples_negative_nooverlap=[]\n",
    "\n",
    "for i in range(0,len(positive_rearing)):\n",
    "    if i not in training_samples_positive:\n",
    "        holdout_samples_positive.append(i)\n",
    "# this foreloop makes sure that there is no repeats\n",
    "for i in range(0,len(negative_rearing)):\n",
    "    if i not in training_samples_negative:\n",
    "        holdout_samples_negative_nooverlap.append(i)\n",
    "\n",
    "indicies=random.sample(range(0,len(holdout_samples_negative_nooverlap)),len(holdout_samples_positive))\n",
    "\n",
    "holdout_samples_negative=[ holdout_samples_negative_nooverlap[i] for i in indicies]\n",
    "\n",
    "# get prediciton data like this--get data from each specific row into a matrix\n",
    "\n",
    "\n",
    "training_samples=np.transpose(np.zeros(6))\n",
    "holdout_data=np.transpose(np.zeros(6))\n",
    "gt_labels_holdout=[]\n",
    "gt_labels_training=[]\n",
    "\n",
    "for i in range(0,len(holdout_samples_negative)):\n",
    "    row_num_neg= holdout_samples_negative[i]\n",
    "    row_num_pos=holdout_samples_positive[i]\n",
    "    holdout_data=np.vstack((holdout_data,negative_instances_pred[row_num_neg]))\n",
    "    holdout_data=np.vstack((holdout_data,positive_instances_pred[row_num_pos]))\n",
    "    gt_labels_holdout=np.append(gt_labels_holdout,0)\n",
    "    gt_labels_holdout=np.append(gt_labels_holdout,1)\n",
    "    \n",
    "holdout_data=np.delete(holdout_data,0,0)\n",
    "\n",
    "\n",
    "    \n",
    "for i in range(0,len(training_samples_positive)):\n",
    "    row_num_positive=training_samples_positive[i]\n",
    "    row_num_negative=training_samples_negative[i]\n",
    "    training_samples=np.vstack((training_samples,positive_instances_pred[row_num_positive]))\n",
    "    training_samples=np.vstack((training_samples,negative_instances_pred[row_num_negative]))\n",
    "    gt_labels_training=np.append(gt_labels_training,1)\n",
    "    gt_labels_training=np.append(gt_labels_training,0)\n",
    "    \n",
    "    \n",
    "training_samples=np.delete(training_samples,0,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we train and evaluate our SVM.We present the accuracy on the holdout testing set as well as a \n",
    "confusion matrix to visualize our misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Testing Accuracy: 0.94\n",
      "Below we have a confusion matrix\n",
      "[[23  2]\n",
      " [ 1 24]]\n"
     ]
    }
   ],
   "source": [
    "# SVM training and Evaluation\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "\n",
    "clf.fit(training_samples,gt_labels_training)\n",
    "\n",
    "predictions=clf.predict(holdout_data)\n",
    "\n",
    "loss=zero_one_loss(gt_labels_holdout, predictions) \n",
    "Accuracy=1-loss\n",
    "\n",
    "\n",
    "print('Overall Testing Accuracy: {}'.format(Accuracy))\n",
    "\n",
    "\n",
    "confusion=confusion_matrix(gt_labels_holdout, predictions)\n",
    "print('Below we have a confusion matrix')\n",
    "print(confusion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVM_prediciton_rearing.joblib']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model if your happy with the results\n",
    "from joblib import dump, load\n",
    "dump(clf, 'SVM_prediciton_rearing.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that while we do get good classification on this trial, applying the model on different rat models does not result in good accuracy and therefore we recomend training the model seperately for each trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
